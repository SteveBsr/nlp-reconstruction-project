{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906d3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline 1: Paraphrasing (Vamsi/T5_Paraphrase_Paws)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 150, but your input_length is only 113. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives today.\n",
      "Hope you enjoy it as my deepest wishes to you too.\n",
      "Thank you for your message to show our words to the doctor, as his next contract checking, to all of us.\n",
      "I got this message to see the approved message .\n",
      "In fact, I have received the message from the professor to show me this a couple of days ago .\n",
      "I am very grateful for the full support of the professor for our Springer proceedings publication .\n",
      "\n",
      "---\n",
      "\n",
      "During our final discussion, I told him about the new submission — the one we were waiting for since last autumn , but the updates were confusing as it did not include the full feedback from reviewer or maybe editor? Anyway, I think the team, although a bit delay and less communication at recent days, really tried for paper and cooperation best .\n",
      "We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link finally came last week, I think .\n",
      "Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sends again .\n",
      "Because I didn’t see that part final yet, or maybe I missed it, I apologize if so .\n",
      "Let us make sure that all are safe and celebrate the outcome with strong coffee and future targets .\n",
      "\n",
      "Pipeline 2: Summarization (facebook/bart-large-cnn)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes.Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
      "\n",
      "---\n",
      "\n",
      "\"We should be grateful, I mean all of us, for the acceptance and efforts,\" he says. \"Let us make sure all are safe and celebrate the outcome with strong coffee and future targets,\" he adds. \"Please remind me please, if the doctor still plan for the acknowledgments section edit before sending again\"\n",
      "\n",
      "Pipeline 3: Paraphrasing (ramsrigouthamg/t5_paraphraser)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives.\n",
      "Hope you too, to enjoy it as my deepest wishes.\n",
      "Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
      "I got this message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved message to see the approved\n",
      "In fact, I have received the message from the professor, to show me, this, a couple of days ago. In fact, I have received the message from the professor, to show me, this, a couple of days ago.\n",
      "I am very appreciated the full support of the professor, for our Springer proceedings publication. I am very appreciated the full support of the professor, for our Springer proceedings publication.\n",
      "\n",
      "---\n",
      "\n",
      "During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation.\n",
      "We should be grateful, I think all of us, for the acceptance and efforts until the Springer link came finally last week, I think, I think. We should be grateful, I think all of us, for the acceptance and efforts until the Springer link came finally last week, I think.\n",
      "Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again.\n",
      "Because I didn’t see that part final yet, or maybe I missed, I apologize if so, I apologize if so. Because I didn’t see that part final yet, or maybe I missed, I apologize if so.\n",
      "Let us make sure all are safe and celebrate the outcome with strong coffee and future targets. Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets. Let us make sure all are safe and celebrate the outcome with strong coffee and future targets.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Τα 2 κείμενα\n",
    "text1 = \"\"\"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes.\n",
    "Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
    "I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago.\n",
    "I am very appreciated the full support of the professor, for our Springer proceedings publication\"\"\"\n",
    "\n",
    "text2 = \"\"\"During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor?\n",
    "Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think.\n",
    "Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so.\n",
    "Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\"\"\"\n",
    "\n",
    "# Χωρίζουμε σε προτάσεις  \n",
    "def split_sentences(text):\n",
    "    return [s.strip() for s in text.replace(\"\\n\", \" \").split('.') if s.strip()]\n",
    "\n",
    "sents1 = split_sentences(text1)\n",
    "sents2 = split_sentences(text2)\n",
    "\n",
    "# Pipeline 1: Paraphrasing T5\n",
    "paraphraser1 = pipeline(\"text2text-generation\", model=\"Vamsi/T5_Paraphrase_Paws\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Pipeline 2: Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# Pipeline 3: Paraphrasing with another T5 paraphrase model\n",
    "paraphraser3 = pipeline(\"text2text-generation\", model=\"ramsrigouthamg/t5_paraphraser\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def reconstruct(sentences, pipeline_fn, max_len=100):\n",
    "    results = []\n",
    "    for sent in sentences:\n",
    "        if len(sent) < 10:\n",
    "            results.append(sent) \n",
    "            continue\n",
    "        out = pipeline_fn(sent, max_length=max_len, num_return_sequences=1)\n",
    "        if isinstance(out, list):\n",
    "            text_out = out[0]['generated_text'] if 'generated_text' in out[0] else out[0]['summary_text']\n",
    "        else:\n",
    "            text_out = out\n",
    "        results.append(text_out)\n",
    "    return results\n",
    "\n",
    "print(\"\\nPipeline 1: Paraphrasing (Vamsi/T5_Paraphrase_Paws)\")\n",
    "recon1_text1 = reconstruct(sents1, paraphraser1)\n",
    "recon1_text2 = reconstruct(sents2, paraphraser1)\n",
    "print(\"\\n\".join(recon1_text1))\n",
    "print(\"\\n---\\n\")\n",
    "print(\"\\n\".join(recon1_text2))\n",
    "\n",
    "print(\"\\nPipeline 2: Summarization (facebook/bart-large-cnn)\")\n",
    "recon2_text1 = reconstruct([text1], summarizer, max_len=150)\n",
    "recon2_text2 = reconstruct([text2], summarizer, max_len=150)\n",
    "print(recon2_text1[0])\n",
    "print(\"\\n---\\n\")\n",
    "print(recon2_text2[0])\n",
    "\n",
    "print(\"\\nPipeline 3: Paraphrasing (ramsrigouthamg/t5_paraphraser)\")\n",
    "recon3_text1 = reconstruct(sents1, paraphraser3)\n",
    "recon3_text2 = reconstruct(sents2, paraphraser3)\n",
    "print(\"\\n\".join(recon3_text1))\n",
    "print(\"\\n---\\n\")\n",
    "print(\"\\n\".join(recon3_text2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-reconstruction-project-2fGmCwJv-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
